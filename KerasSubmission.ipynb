{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize libraries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/home/alokkumary2j/anaconda3/lib/python3.5/site-packages/xgboost/./lib/libxgboost.so: invalid ELF header",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-abde97afa227>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_activations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alokkumary2j/anaconda3/lib/python3.5/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrabit\u001b[0m                   \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alokkumary2j/anaconda3/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;31m# load the XGBoost library globally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m \u001b[0m_LIB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alokkumary2j/anaconda3/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mlib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alokkumary2j/anaconda3/lib/python3.5/ctypes/__init__.py\u001b[0m in \u001b[0;36mLoadLibrary\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[0mcdll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLibraryLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alokkumary2j/anaconda3/lib/python3.5/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: /home/alokkumary2j/anaconda3/lib/python3.5/site-packages/xgboost/./lib/libxgboost.so: invalid ELF header"
     ]
    }
   ],
   "source": [
    "# Bag of apps categories\n",
    "# Bag of labels categories\n",
    "# Include phone brand and model device\n",
    "\n",
    "print(\"Initialize libraries\")\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\n",
    "from sklearn import ensemble\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#------------------------------------------------- Write functions ----------------------------------------\n",
    "\n",
    "def rstr(df): return df.dtypes, df.head(3) ,df.apply(lambda x: [x.unique()]), df.apply(lambda x: [len(x.unique())]),df.shape\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0\n",
    "\n",
    "#------------------------------------------------ Read data from source files ------------------------------------\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "datadir = '../input'\n",
    "\n",
    "print(\"### ----- PART 1 ----- ###\")\n",
    "\n",
    "# Data - Events data\n",
    "# Bag of apps\n",
    "print(\"# Read app events\")\n",
    "app_events = pd.read_csv(os.path.join(datadir,'app_events.csv'), dtype={'device_id' : np.str})\n",
    "app_events.head(5)\n",
    "app_events.info()\n",
    "#print(rstr(app_events))\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "app_events= app_events.groupby(\"event_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n",
    "app_events.head(5)\n",
    "\n",
    "print(\"# Read Events\")\n",
    "events = pd.read_csv(os.path.join(datadir,'events.csv'), dtype={'device_id': np.str})\n",
    "events.head(5)\n",
    "events[\"app_id\"] = events[\"event_id\"].map(app_events)\n",
    "events = events.dropna()\n",
    "del app_events\n",
    "\n",
    "events = events[[\"device_id\", \"app_id\"]]\n",
    "events.info()\n",
    "# 1Gb reduced to 34 Mb\n",
    "\n",
    "# remove duplicates(app_id)\n",
    "events.loc[:,\"device_id\"].value_counts(ascending=True)\n",
    "\n",
    "events = events.groupby(\"device_id\")[\"app_id\"].apply(\n",
    "    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\n",
    "events = events.reset_index(name=\"app_id\")\n",
    "\n",
    "# expand to multiple rows\n",
    "events = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n",
    "                    for _, row in events.iterrows()]).reset_index()\n",
    "events.columns = ['app_id', 'device_id']\n",
    "events.head(5)\n",
    "f3 = events[[\"device_id\", \"app_id\"]]    # app_id\n",
    "\n",
    "print(\"#Part1 formed\")\n",
    "\n",
    "##################\n",
    "#   App labels\n",
    "##################\n",
    "\n",
    "print(\"### ----- PART 2 ----- ###\")\n",
    "\n",
    "print(\"# Read App labels\")\n",
    "app_labels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\n",
    "label_cat = pd.read_csv(os.path.join(datadir,'label_categories.csv'))\n",
    "app_labels.info()\n",
    "label_cat.info()\n",
    "label_cat=label_cat[['label_id','category']]\n",
    "\n",
    "app_labels=app_labels.merge(label_cat,on='label_id',how='left')\n",
    "app_labels.head(3)\n",
    "events.head(3)\n",
    "#app_labels = app_labels.loc[app_labels.smaller_cat != \"unknown_unknown\"]\n",
    "\n",
    "#app_labels = app_labels.groupby(\"app_id\")[\"category\"].apply(\n",
    "#    lambda x: \";\".join(set(\"app_cat:\" + str(s) for s in x)))\n",
    "app_labels = app_labels.groupby([\"app_id\",\"category\"]).agg('size').reset_index()\n",
    "app_labels = app_labels[['app_id','category']]\n",
    "print(\"# App labels done\")\n",
    "\n",
    "\n",
    "# Remove \"app_id:\" from column\n",
    "print(\"## Handling events data for merging with app lables\")\n",
    "events['app_id'] = events['app_id'].map(lambda x : x.lstrip('app_id:'))\n",
    "events['app_id'] = events['app_id'].astype(str)\n",
    "app_labels['app_id'] = app_labels['app_id'].astype(str)\n",
    "app_labels.info()\n",
    "\n",
    "print(\"## Merge\")\n",
    "\n",
    "events= pd.merge(events, app_labels, on = 'app_id',how='left').astype(str)\n",
    "#events['smaller_cat'].unique()\n",
    "\n",
    "# expand to multiple rows\n",
    "print(\"#Expand to multiple rows\")\n",
    "#events= pd.concat([pd.Series(row['device_id'], row['category'].split(';'))\n",
    "#                    for _, row in events.iterrows()]).reset_index()\n",
    "#events.columns = ['app_cat', 'device_id']\n",
    "#events.head(5)\n",
    "#print(events.info())\n",
    "\n",
    "events= events.groupby([\"device_id\",\"category\"]).agg('size').reset_index()\n",
    "events= events[['device_id','category']]\n",
    "events.head(10)\n",
    "print(\"# App labels done\")\n",
    "\n",
    "f5 = events[[\"device_id\", \"category\"]]    # app_id\n",
    "# Can % total share be included as well?\n",
    "print(\"# App category part formed\")\n",
    "\n",
    "##################\n",
    "#   Phone Brand\n",
    "##################\n",
    "print(\"### ----- PART 3 ----- ###\")\n",
    "\n",
    "print(\"# Read Phone Brand\")\n",
    "pbd = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'),\n",
    "                  dtype={'device_id': np.str})\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "##################\n",
    "#  Train and Test\n",
    "##################\n",
    "print(\"# Generate Train and Test\")\n",
    "\n",
    "train = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n",
    "                    dtype={'device_id': np.str})\n",
    "train.drop([\"age\", \"gender\"], axis=1, inplace=True)\n",
    "\n",
    "test = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n",
    "                   dtype={'device_id': np.str})\n",
    "test[\"group\"] = np.nan\n",
    "\n",
    "\n",
    "split_len = len(train)\n",
    "\n",
    "# Group Labels\n",
    "Y = train[\"group\"]\n",
    "lable_group = LabelEncoder()\n",
    "Y = lable_group.fit_transform(Y)\n",
    "device_id = test[\"device_id\"]\n",
    "\n",
    "# Concat\n",
    "Df = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "print(\"### ----- PART 4 ----- ###\")\n",
    "\n",
    "Df = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\n",
    "Df[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\n",
    "Df[\"device_model\"] = Df[\"device_model\"].apply(\n",
    "    lambda x: \"device_model:\" + str(x))\n",
    "\n",
    "\n",
    "###################\n",
    "#  Concat Feature\n",
    "###################\n",
    "\n",
    "print(\"# Concat all features\")\n",
    "\n",
    "f1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\n",
    "f2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n",
    "\n",
    "events = None\n",
    "Df = None\n",
    "\n",
    "f1.columns.values[1] = \"feature\"\n",
    "f2.columns.values[1] = \"feature\"\n",
    "f5.columns.values[1] = \"feature\"\n",
    "f3.columns.values[1] = \"feature\"\n",
    "\n",
    "FLS = pd.concat((f1, f2, f3, f5), axis=0, ignore_index=True)\n",
    "\n",
    "FLS.info()\n",
    "\n",
    "###################\n",
    "# User-Item Feature\n",
    "###################\n",
    "print(\"# User-Item-Feature\")\n",
    "\n",
    "device_ids = FLS[\"device_id\"].unique()\n",
    "feature_cs = FLS[\"feature\"].unique()\n",
    "\n",
    "data = np.ones(len(FLS))\n",
    "len(data)\n",
    "\n",
    "dec = LabelEncoder().fit(FLS[\"device_id\"])\n",
    "row = dec.transform(FLS[\"device_id\"])\n",
    "col = LabelEncoder().fit_transform(FLS[\"feature\"])\n",
    "sparse_matrix = sparse.csr_matrix(\n",
    "    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\n",
    "sparse_matrix.shape\n",
    "sys.getsizeof(sparse_matrix)\n",
    "\n",
    "sparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\n",
    "print(\"# Sparse matrix done\")\n",
    "\n",
    "del FLS\n",
    "del data\n",
    "f1 = [1]\n",
    "f5 = [1]\n",
    "f2 = [1]\n",
    "f3 = [1]\n",
    "\n",
    "events = [1]\n",
    "\n",
    "##################\n",
    "#      Data\n",
    "##################\n",
    "\n",
    "print(\"# Split data\")\n",
    "train_row = dec.transform(train[\"device_id\"])\n",
    "train_sp = sparse_matrix[train_row, :]\n",
    "\n",
    "test_row = dec.transform(test[\"device_id\"])\n",
    "test_sp = sparse_matrix[test_row, :]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sp, Y, train_size=0.999, random_state=10)\n",
    "\n",
    "##################\n",
    "#   Feature Sel\n",
    "##################\n",
    "print(\"# Feature Selection\")\n",
    "#selector = SelectPercentile(f_classif, percentile=53)\n",
    "\n",
    "#selector.fit(X_train, y_train)\n",
    "#X_train.shape\n",
    "#X_train = selector.transform(X_train)\n",
    "#X_train.shape\n",
    "#X_val = selector.transform(X_val)\n",
    "#X_val.shape\n",
    "\n",
    "# Selection using chi-square\n",
    "# selector = SelectKBest(chi2, k=11155).fit(X_train, y_train)\n",
    "# X_train.shape\n",
    "# X_train = selector.transform(X_train)\n",
    "# X_train.shape\n",
    "# X_val = selector.transform(X_val)\n",
    "# X_val.shape\n",
    "\n",
    "print(\"# Num of Features: \", X_train.shape[1])\n",
    "\n",
    "##################\n",
    "#  Build Model\n",
    "##################\n",
    "\n",
    "\n",
    "#act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model\n",
    "\n",
    "model=baseline_model()\n",
    "\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n",
    "                         nb_epoch=16,\n",
    "                         samples_per_epoch=69984,\n",
    "                         validation_data=(X_val.todense(), y_val), verbose=2\n",
    "                         )\n",
    "\n",
    "# evaluate the model\n",
    "scores_val = model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\n",
    "print('logloss val {}'.format(log_loss(y_val, scores_val)))\n",
    "\n",
    "print(\"# Final prediction\")\n",
    "scores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\n",
    "result = pd.DataFrame(scores , columns=lable_group.classes_)\n",
    "result[\"device_id\"] = device_id\n",
    "print(result.head(1))\n",
    "result = result.set_index(\"device_id\")\n",
    "\n",
    "#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n",
    "#Drop out 0.2\n",
    "#Validation 2.3017\n",
    "result.to_csv('sub_bagofapps7_keras_150_pt4_50_pt2_15epoch_prelu_softmax.csv', index=True, index_label='device_id')\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
